{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "CWchO-TRUSUG",
    "outputId": "d15db3b8-fdf1-4920-f575-1a8c2a9a3b1a"
   },
   "outputs": [],
   "source": [
    "!pip install openai pinecone-client tiktoken\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import tiktoken\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeAclPlrUSWy"
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and model for embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-small-v2\")\n",
    "model = AutoModel.from_pretrained(\"intfloat/e5-small-v2\")\n",
    "\n",
    "# API Keys (replace with your actual keys)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj\"\n",
    "PINECONE_API_KEY = \"PINECONE_API_\"\n",
    "\n",
    "# Variables\n",
    "INDEX_NAME = \"ragchatbot\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "MAX_TOKENS = 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7MBd1jhUSZc"
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model(GPT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVpUlE1UUSce"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Store conversation history in an array\n",
    "conversation = [\n",
    "    \"1: User: Hi there! How are you doing today? | Bot: Hello! I'm doing great, thank you! How can I assist you today?\",\n",
    "    \"2: User: What's the weather like today in New York? | Bot: Today in New York, it's sunny with a slight chance of rain.\",\n",
    "    \"3: User: Great! Do you have any good lunch suggestions? | Bot: Sure! How about trying a new salad recipe?\",\n",
    "    \"4: User: That sounds healthy. Any specific recipes? | Bot: You could try a quinoa salad with avocado and chicken.\",\n",
    "    \"5: User: Sounds delicious! I'll try it. What about dinner? | Bot: For dinner, you could make grilled salmon with vegetables.\",\n",
    "    \"6: User: Thanks for the suggestions! Any dessert ideas? | Bot: How about a simple fruit salad or yogurt with honey?\",\n",
    "    \"7: User: Perfect! Now, what are some good exercises? | Bot: You can try a mix of cardio and strength training exercises.\",\n",
    "    \"8: User: Any specific recommendations for cardio? | Bot: Running, cycling, and swimming are all excellent cardio exercises.\",\n",
    "    \"9: User: I'll start with running. Can you recommend any books? | Bot: 'Atomic Habits' by James Clear is a highly recommended book.\",\n",
    "    \"10: User: I'll check it out. What hobbies can I take up? | Bot: You could explore painting, hiking, or learning a new instrument.\",\n",
    "    \"11: User: Hiking sounds fun! Any specific trails? | Bot: There are great trails in the Rockies and the Appalachian Mountains.\",\n",
    "    \"12: User: I'll plan a trip. What about indoor activities? | Bot: Indoor activities like reading, cooking, or playing board games.\",\n",
    "    \"13: User: Nice! Any good board games? | Bot: Settlers of Catan and Ticket to Ride are both excellent choices.\",\n",
    "    \"14: User: I'll try them out. Any movie recommendations? | Bot: 'Inception' and 'The Matrix' are must-watch movies.\",\n",
    "    \"15: User: I love those movies! Any TV shows? | Bot: 'Breaking Bad' and 'Stranger Things' are very popular.\",\n",
    "    \"16: User: Great choices! What about podcasts? | Bot: 'How I Built This' and 'The Daily' are very informative.\",\n",
    "    \"17: User: Thanks! What are some good travel destinations? | Bot: Paris, Tokyo, and Bali are amazing travel spots.\",\n",
    "    \"18: User: I'll add them to my list. Any packing tips? | Bot: Roll your clothes to save space and use packing cubes.\",\n",
    "    \"19: User: That's helpful! What about travel insurance? | Bot: Always get travel insurance for safety and peace of mind.\",\n",
    "    \"20: User: Thanks for the tips! Any last advice? | Bot: Just enjoy your journey and make the most out of your experiences.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbhSbgZQf-hP"
   },
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Function to store chat history in Pinecone in chunks\n",
    "def store_chat_history(conversation, batch_size=10):\n",
    "    chunk = []\n",
    "    for entry in conversation:\n",
    "        parts = entry.split(\": \", 2)\n",
    "        if len(parts) == 3:\n",
    "            id, user_message, bot_message = parts\n",
    "            full_message = f\"User: {user_message} Bot: {bot_message}\"\n",
    "            embedding = get_embedding(full_message).tolist()\n",
    "            chunk.append((id, embedding, {\"text\": full_message}))\n",
    "\n",
    "            # When chunk reaches batch size, upsert into Pinecone\n",
    "            if len(chunk) == batch_size:\n",
    "                index.upsert(vectors=chunk)\n",
    "                chunk = []\n",
    "\n",
    "    # Upsert any remaining vectors in the final chunk\n",
    "    if chunk:\n",
    "        index.upsert(vectors=chunk)\n",
    "\n",
    "# Store the conversation history\n",
    "store_chat_history(conversation)\n",
    "\n",
    "# Cell 5: Define the RAG mechanism\n",
    "def fetch_relevant_messages(query, top_k=3):\n",
    "    query_embedding = get_embedding(query).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "    print(results)\n",
    "    return [match['metadata']['text'] for match in results['matches']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jto5GJFiPF4o"
   },
   "outputs": [],
   "source": [
    "#Retrive from pinecone\n",
    "def test_retrieve_and_print_vectors(query):\n",
    "    relevant_history_with_vectors = fetch_relevant_messages(query)\n",
    "\n",
    "    print(\"Relevant History and Vectors:\")\n",
    "    for text in relevant_history_with_vectors:\n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"------\")\n",
    "\n",
    "# Test the function with a query\n",
    "test_query = \"Do you think it will help me stay fit?\"\n",
    "test_retrieve_and_print_vectors(test_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etKoe-0VKP9Y"
   },
   "outputs": [],
   "source": [
    "# Function to generate response using relevant history\n",
    "def generate_response(query, relevant_history):\n",
    "    context = \"\\n\".join(relevant_history)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuery: {query}\\nResponse:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=MAX_TOKENS - count_tokens(prompt)\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Function to process user input\n",
    "def process_user_input(user_input):\n",
    "    relevant_history = fetch_relevant_messages(user_input)\n",
    "    response = generate_response(user_input, relevant_history)\n",
    "    return relevant_history, response\n",
    "\n",
    "# Cell 6: Testing the implementation\n",
    "\n",
    "def test_final_prompt():\n",
    "    final_test_prompt = \"What was that book recommendation again?\"\n",
    "    relevant_history, response = process_user_input(final_test_prompt)\n",
    "\n",
    "    print(f\"Final Test Prompt: {final_test_prompt}\")\n",
    "    print(f\"Context Referred: {relevant_history}\")\n",
    "    print(f\"Final Test Prompt Response: {response}\")\n",
    "\n",
    "    total_tokens = count_tokens(\"\\n\".join(relevant_history) + response)\n",
    "    print(f\"\\nTotal tokens (context + response): {total_tokens}\")\n",
    "\n",
    "# Call the test function to generate the Final Test Prompt Response\n",
    "test_final_prompt()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
